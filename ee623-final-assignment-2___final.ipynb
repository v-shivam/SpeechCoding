{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10011638,"sourceType":"datasetVersion","datasetId":6163318}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport scipy.signal as signal\nimport librosa\nimport soundfile as sf\nfrom scipy.linalg import toeplitz\nimport os\nimport time\nimport numpy as np\n\ndef segsnr(reference_signal, synthesized_signal, fs, frame_size=0.03, overlap=0.5):\n    \"\"\"\n    Calculate Segmental Signal-to-Noise Ratio (SNR) in dB.\n\n    Parameters:\n        reference_signal (numpy array): Original clean signal.\n        synthesized_signal (numpy array): Processed signal.\n        fs (int): Sampling frequency.\n        frame_size (float): Frame size in seconds (default 30 ms).\n        overlap (float): Overlap fraction between consecutive frames (default 50%).\n\n    Returns:\n        float: Segmental SNR in dB.\n    \"\"\"\n    frame_length = int(frame_size * fs)\n    hop_length = int(frame_length * (1 - overlap))\n    num_frames = int(np.ceil((len(reference_signal) - frame_length) / hop_length)) + 1\n\n    padded_reference = np.append(reference_signal, np.zeros(num_frames * hop_length + frame_length - len(reference_signal)))\n    padded_synthesized = np.append(synthesized_signal, np.zeros(num_frames * hop_length + frame_length - len(synthesized_signal)))\n\n    window = np.hamming(frame_length)\n    seg_snr_values = []\n\n    for i in range(num_frames):\n        start_idx = i * hop_length\n        end_idx = start_idx + frame_length\n        ref_frame = padded_reference[start_idx:end_idx] * window\n        synth_frame = padded_synthesized[start_idx:end_idx] * window\n\n        signal_energy = np.sum(ref_frame ** 2)\n        noise_energy = np.sum((ref_frame - synth_frame) ** 2)\n\n        if signal_energy > 0 and noise_energy > 0:\n            seg_snr = 10 * np.log10(signal_energy / noise_energy)\n            seg_snr_values.append(seg_snr)\n\n    if len(seg_snr_values) > 0:\n        return np.mean(seg_snr_values)\n    else:\n        return float('-inf')  # If no valid frames, return -inf as a placeholder\n\n# Function to compute LPC coefficients with stability check\ndef lpc_analysis(frame, order):\n    autocorr = np.correlate(frame, frame, mode='full')\n    autocorr = autocorr[len(autocorr) // 2:]\n    R = autocorr[:order + 1]\n    try:\n        A = np.linalg.solve(toeplitz(R[:-1]), -R[1:])\n    except np.linalg.LinAlgError:\n        A = np.zeros(order)\n    A = np.concatenate(([1], A))\n    if not is_stable(A):\n        A = np.array([1] + [0] * order)\n    G = np.sqrt(R[0] + np.dot(R[1:], A[1:]))\n    return A, G\n\n# Function to check LPC filter stability\ndef is_stable(A):\n    roots = np.roots(A)\n    return np.all(np.abs(roots) < 1)\n\n# Function to generate excitation signal\ndef generate_excitation(frame_length, voiced, pitch_period=None):\n    if voiced and pitch_period is not None and pitch_period > 0:\n        excitation = np.zeros(frame_length)\n        excitation[::pitch_period] = 1.0\n    else:\n        excitation = np.random.randn(frame_length)\n    return excitation\n\n# Function to determine if a frame is voiced\ndef is_voiced(frame, fs):\n    energy = np.sum(frame ** 2) / len(frame)\n    zcr = ((frame[:-1] * frame[1:]) < 0).sum() / len(frame)\n    threshold_energy = 0.02\n    threshold_zcr = 0.1\n    return energy > threshold_energy and zcr < threshold_zcr\n\n# Function to estimate pitch\ndef estimate_pitch(frame, fs, min_freq=60, max_freq=400):\n    min_lag = int(fs / max_freq)\n    max_lag = int(fs / min_freq)\n    autocorr = np.correlate(frame, frame, mode='full')\n    autocorr = autocorr[len(autocorr) // 2:]\n    autocorr[:min_lag] = 0\n    autocorr[max_lag + 1:] = 0\n    peaks, _ = signal.find_peaks(autocorr)\n    if len(peaks) > 0:\n        return peaks[np.argmax(autocorr[peaks])]\n    return None\n\n# Main LPC Vocoder function\nimport time\n\ndef lpc_vocoder(input_file, output_file, lpc_order=12, frame_size=0.03, fs=None, overlap=0.5, mode=\"plain\"):\n    start_time = time.time()\n    speech, sr = librosa.load(input_file, sr=fs)\n    if fs is None:\n        fs = sr\n\n    pre_emphasis = 0.97\n    emphasized_speech = np.append(speech[0], speech[1:] - pre_emphasis * speech[:-1])\n    frame_length = int(frame_size * fs)\n    hop_length = int(frame_length * (1 - overlap))\n    num_frames = int(np.ceil((len(emphasized_speech) - frame_length) / hop_length)) + 1\n    padded_speech = np.append(emphasized_speech, np.zeros(num_frames * hop_length + frame_length - len(emphasized_speech)))\n    synthesized_speech = np.zeros(len(padded_speech))\n    window = np.hamming(frame_length)\n\n    for i in range(num_frames):\n        start_idx = i * hop_length\n        end_idx = start_idx + frame_length\n        frame = padded_speech[start_idx:end_idx] * window\n        voiced = is_voiced(frame, fs)\n        A, G = lpc_analysis(frame, lpc_order)\n        pitch_period = estimate_pitch(frame, fs) if voiced else None\n        excitation = generate_excitation(frame_length, voiced, pitch_period)\n        if mode == \"voice-excited\":\n            excitation = signal.lfilter([1, -0.8], [1], excitation)\n        synthesized_frame = signal.lfilter([1], A, excitation) * G\n        synthesized_speech[start_idx:end_idx] += synthesized_frame * window\n\n    synthesized_speech = signal.lfilter([1], [1, -pre_emphasis], synthesized_speech)\n    max_val = np.max(np.abs(synthesized_speech))\n    if max_val > 0:\n        synthesized_speech = synthesized_speech / max_val * 0.99\n\n    sf.write(output_file, synthesized_speech[:len(speech)], fs)\n\n    end_time = time.time()\n    runtime = end_time - start_time\n\n    # Calculate bit rate\n    bits_per_lpc_coeff = 16  # bits per LPC coefficient\n    bits_per_pitch_period = 16  # bits per pitch period\n    bits_per_frame = lpc_order * bits_per_lpc_coeff\n    total_bits = num_frames * bits_per_frame\n\n    # Add bits for pitch periods in voiced frames\n    for i in range(num_frames):\n        start_idx = i * hop_length\n        end_idx = start_idx + frame_length\n        frame = padded_speech[start_idx:end_idx] * window\n        if is_voiced(frame, fs):\n            total_bits += bits_per_pitch_period\n\n    total_seconds = len(speech) / fs\n    bit_rate = total_bits / total_seconds\n\n    return bit_rate, runtime\n\nif __name__ == \"__main__\":\n    fs = 16000\n    lpc_order = 10\n    frame_size = 0.03\n\n    speech_files = [\n        \"/kaggle/input/speechdata/female1.wav\",\n        \"/kaggle/input/speechdata/male1.wav\",\n        \"/kaggle/input/speechdata/female2.wav\",\n        \"/kaggle/input/speechdata/male2.wav\"\n    ]\n\n    results = []\n    seg_snr_results = []\n    for input_file in speech_files:\n        if not os.path.isfile(input_file):\n            print(f\"File {input_file} not found.\")\n            continue\n        \n        speech, _ = librosa.load(input_file, sr=fs)\n        \n        # Process Plain LPC Vocoder\n        output_file_plain = os.path.splitext(os.path.basename(input_file))[0] + \"_plain_lpc.wav\"\n        bit_rate, runtime = lpc_vocoder(input_file, output_file_plain, lpc_order, frame_size, fs, mode=\"plain\")\n        synthesized_plain, _ = librosa.load(output_file_plain, sr=fs)\n        plain_segsnr = segsnr(speech, synthesized_plain, fs, frame_size)\n        results.append((input_file, \"plain\", bit_rate, runtime, plain_segsnr))\n        print(f\"Processed Plain LPC Vocoder: {input_file}, Bit rate: {bit_rate:.2f}, Runtime: {runtime:.2f} seconds, SegSNR: {plain_segsnr:.2f} dB\")\n\n        # Process Voice-excited LPC Vocoder\n        output_file_voice_excited = os.path.splitext(os.path.basename(input_file))[0] + \"_voice_excited_lpc.wav\"\n        bit_rate, runtime = lpc_vocoder(input_file, output_file_voice_excited, lpc_order, frame_size, fs, mode=\"voice-excited\")\n        synthesized_voice_excited, _ = librosa.load(output_file_voice_excited, sr=fs)\n        voice_excited_segsnr = segsnr(speech, synthesized_voice_excited, fs, frame_size)\n        results.append((input_file, \"voice-excited\", bit_rate, runtime, voice_excited_segsnr))\n        print(f\"Processed Voice-excited LPC Vocoder: {input_file}, Bit rate: {bit_rate:.2f}, Runtime: {runtime:.2f} seconds, SegSNR: {voice_excited_segsnr:.2f} dB\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T18:04:09.667721Z","iopub.execute_input":"2024-11-25T18:04:09.668107Z","iopub.status.idle":"2024-11-25T18:04:10.823730Z","shell.execute_reply.started":"2024-11-25T18:04:09.668070Z","shell.execute_reply":"2024-11-25T18:04:10.822245Z"}},"outputs":[{"name":"stdout","text":"Processed Plain LPC Vocoder: /kaggle/input/speechdata/female1.wav, Bit rate: 10640.00, Runtime: 0.14 seconds, SegSNR: -5.40 dB\nProcessed Voice-excited LPC Vocoder: /kaggle/input/speechdata/female1.wav, Bit rate: 10640.00, Runtime: 0.18 seconds, SegSNR: -3.20 dB\nProcessed Plain LPC Vocoder: /kaggle/input/speechdata/male1.wav, Bit rate: 10639.67, Runtime: 0.05 seconds, SegSNR: -4.54 dB\nProcessed Voice-excited LPC Vocoder: /kaggle/input/speechdata/male1.wav, Bit rate: 10639.67, Runtime: 0.07 seconds, SegSNR: -3.52 dB\nProcessed Plain LPC Vocoder: /kaggle/input/speechdata/female2.wav, Bit rate: 10639.83, Runtime: 0.10 seconds, SegSNR: -2.66 dB\nProcessed Voice-excited LPC Vocoder: /kaggle/input/speechdata/female2.wav, Bit rate: 10639.83, Runtime: 0.13 seconds, SegSNR: -5.40 dB\nProcessed Plain LPC Vocoder: /kaggle/input/speechdata/male2.wav, Bit rate: 10666.56, Runtime: 0.15 seconds, SegSNR: -3.66 dB\nProcessed Voice-excited LPC Vocoder: /kaggle/input/speechdata/male2.wav, Bit rate: 10666.56, Runtime: 0.18 seconds, SegSNR: -3.46 dB\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!pip install pesq\nimport numpy as np\nimport scipy.signal as signal\nimport librosa\nimport soundfile as sf\nfrom sklearn.cluster import KMeans\nfrom scipy.linalg import toeplitz\nimport os\n\n# Function to compute LPC coefficients with stability check\ndef lpc_analysis(frame, order):\n    autocorr = np.correlate(frame, frame, mode='full')\n    autocorr = autocorr[len(autocorr) // 2:]\n    R = autocorr[:order + 1]\n    try:\n        A = np.linalg.solve(toeplitz(R[:-1]), -R[1:])\n    except np.linalg.LinAlgError:\n        A = np.zeros(order)\n    A = np.concatenate(([1], A))\n    if not is_stable(A):\n        A = np.array([1] + [0] * order)\n    G = np.sqrt(R[0] + np.dot(R[1:], A[1:]))\n    return A, G\n\n# Function to check LPC filter stability\ndef is_stable(A):\n    roots = np.roots(A)\n    return np.all(np.abs(roots) < 1)\n\n# Function to determine if a frame is voiced\ndef is_voiced(frame, fs):\n    energy = np.sum(frame ** 2) / len(frame)\n    zcr = ((frame[:-1] * frame[1:]) < 0).sum() / len(frame)\n    threshold_energy = 0.01\n    threshold_zcr = 0.15\n    return energy > threshold_energy and zcr < threshold_zcr\n\n# Refined function to estimate pitch\ndef estimate_pitch(frame, fs, min_freq=60, max_freq=400):\n    min_lag = int(fs / max_freq)\n    max_lag = int(fs / min_freq)\n    autocorr = np.correlate(frame, frame, mode='full')\n    autocorr = autocorr[len(autocorr) // 2:]\n    autocorr[:min_lag] = 0\n    autocorr[max_lag + 1:] = 0\n    peaks, _ = signal.find_peaks(autocorr)\n    if len(peaks) > 0:\n        pitch_period = peaks[np.argmax(autocorr[peaks])]\n        return fs / pitch_period\n    return None\n\n# Improved gain computation to ensure energy consistency\ndef compute_gain(residual, subframe):\n    return np.sqrt(np.sum(subframe**2) / (np.sum(residual**2) + 1e-8))\n\n# Function to build the codebook\ndef build_codebook(training_files, fs, lpc_order, frame_size, subframe_size, num_codebook_entries=128):\n    residuals, frame_length, subframe_length = [], int(frame_size * fs), int(subframe_size * fs)\n    for file in training_files:\n        speech, _ = librosa.load(file, sr=fs)\n        emphasized = np.append(speech[0], speech[1:] - 0.97 * speech[:-1])\n        for i in range(len(emphasized) // frame_length):\n            frame = emphasized[i*frame_length:(i+1)*frame_length]\n            A, _ = lpc_analysis(frame, lpc_order)\n            residual = signal.lfilter(A, [1], frame)\n            for j in range(0, frame_length, subframe_length):\n                subframe = residual[j:j+subframe_length]\n                if len(subframe) == subframe_length:\n                    residuals.append(subframe)\n    residuals = np.array(residuals).reshape(len(residuals), -1)\n    kmeans = KMeans(n_clusters=num_codebook_entries, random_state=0)\n    kmeans.fit(residuals)\n    return kmeans.cluster_centers_\n\n# CELP Encoder\ndef celp_encoder(speech, fs, lpc_order, frame_size, subframe_size, codebook):\n    frame_length, subframe_length = int(frame_size * fs), int(subframe_size * fs)\n    num_frames = len(speech) // frame_length\n    lpc_coeffs, codebook_indices, gains = [], [], []\n    for i in range(num_frames):\n        frame = speech[i*frame_length:(i+1)*frame_length]\n        A, G = lpc_analysis(frame, lpc_order)\n        lpc_coeffs.append(A)\n        for j in range(0, frame_length, subframe_length):\n            subframe = frame[j:j+subframe_length]\n            residual = signal.lfilter(A, [1], subframe)\n            distances = np.linalg.norm(codebook - residual, axis=1)\n            best_match = np.argmin(distances)\n            codebook_indices.append(best_match)\n            gains.append(compute_gain(codebook[best_match], subframe))\n    return lpc_coeffs, codebook_indices, gains\n\n# CELP Decoder\ndef celp_decoder(lpc_coeffs, codebook_indices, gains, fs, frame_size, subframe_size, codebook):\n    frame_length, subframe_length = int(frame_size * fs), int(subframe_size * fs)\n    synthesized_speech = []\n    idx = 0\n    for A in lpc_coeffs:\n        frame_synth = []\n        for j in range(0, frame_length, subframe_length):\n            excitation = codebook[codebook_indices[idx]] * gains[idx]\n            frame_synth.extend(signal.lfilter([1], A, excitation))\n            idx += 1\n        synthesized_speech.extend(frame_synth)\n    return np.array(synthesized_speech)\n\n# CELP Codec\ndef celp_codec(input_file, output_file, codebook, lpc_order=14, frame_size=0.03, subframe_size=0.01, fs=None):\n    speech, sr = librosa.load(input_file, sr=fs)\n    fs = fs or sr\n    emphasized = np.append(speech[0], speech[1:] - 0.97 * speech[:-1])\n    lpc_coeffs, codebook_indices, gains = celp_encoder(emphasized, fs, lpc_order, frame_size, subframe_size, codebook)\n    synthesized = celp_decoder(lpc_coeffs, codebook_indices, gains, fs, frame_size, subframe_size, codebook)\n    synthesized = signal.lfilter([1], [1, -0.97], synthesized)\n    sf.write(output_file, synthesized, fs)\n    \n    # Calculate bitrate\n    total_bits = len(codebook_indices) * np.log2(len(codebook)) + len(lpc_coeffs) * lpc_order * 32\n    duration = len(speech) / fs\n    bitrate = total_bits / duration\n    print(f\"Bitrate: {bitrate:.2f} bits/sec\")\n    \n    # Calculate SNR\n    signal_energy = np.sum(speech ** 2)\n    # noise_energy = np.sum((speech - synthesized[:len(speech)]) ** 2)\n    # Ensure both signals are the same length for calculation\n    min_length = min(len(speech), len(synthesized))\n    noise_energy = np.sum((speech[:min_length] - synthesized[:min_length]) ** 2)\n    snr = 10 * np.log10(signal_energy / noise_energy)\n    print(f\"Signal-to-Noise Ratio (SNR): {snr:.2f} dB\")\n    \n    return synthesized\n\nimport time\n\ndef calculate_segmental_snr(original, synthesized, frame_length):\n    num_frames = len(original) // frame_length\n    segmental_snrs = []\n    for i in range(num_frames):\n        orig_frame = original[i * frame_length:(i + 1) * frame_length]\n        synth_frame = synthesized[i * frame_length:(i + 1) * frame_length]\n        noise = orig_frame - synth_frame\n        signal_energy = np.sum(orig_frame ** 2)\n        noise_energy = np.sum(noise ** 2)\n        if noise_energy > 0:\n            segmental_snrs.append(10 * np.log10(signal_energy / noise_energy))\n    return np.mean(segmental_snrs)\n\n# Updated main function to include runtime and segmental SNR\nif __name__ == \"__main__\":\n    fs = 16000\n    lpc_order = 12\n    frame_size = 0.04\n    subframe_size = 0.005\n\n    training_files = [\"/kaggle/input/speechdata/female1.wav\", \"/kaggle/input/speechdata/female2.wav\",\n                      \"/kaggle/input/speechdata/male1.wav\", \"/kaggle/input/speechdata/male2.wav\"]\n    print(\"Building codebook...\")\n    codebook = build_codebook(training_files, fs, lpc_order, frame_size, subframe_size)\n    print(\"Codebook built successfully.\")\n\n    results = []\n\n    for input_file in training_files:\n        original_speech, _ = librosa.load(input_file, sr=fs)\n        output_file = os.path.splitext(os.path.basename(input_file))[0] + \"_celp.wav\"\n\n        print(f\"Processing: {input_file}\")\n        start_time = time.time()\n\n        synthesized_speech_celp = celp_codec(input_file, output_file, codebook, lpc_order, frame_size, subframe_size, fs=fs)\n\n        end_time = time.time()\n        runtime = end_time - start_time\n\n        # Calculate Segmental SNR\n        frame_length = int(frame_size * fs)\n        segmental_snr = calculate_segmental_snr(original_speech, synthesized_speech_celp, frame_length)\n\n        # Calculate PESQ Score\n        try:\n            from pesq import pesq\n            pesq_score = pesq(fs, original_speech[:len(synthesized_speech_celp)], synthesized_speech_celp, 'nb')\n        except Exception as e:\n            pesq_score = None\n            print(f\"Error calculating PESQ score: {e}\")\n\n        results.append({\n            \"file\": input_file,\n            \"runtime (s)\": runtime,\n            \"segmental SNR (dB)\": segmental_snr,\n            \"PESQ Score\": pesq_score\n        })\n\n    # Print results\n    print(\"\\nSummary of Results:\")\n    for result in results:\n        print(f\"File: {result['file']}\")\n        print(f\"Runtime: {result['runtime (s)']:.2f} seconds\")\n        print(f\"Segmental SNR: {result['segmental SNR (dB)']:.2f} dB\")\n        print(f\"PESQ Score: {result['PESQ Score']}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T18:10:34.677730Z","iopub.execute_input":"2024-11-25T18:10:34.678577Z","iopub.status.idle":"2024-11-25T18:10:48.811702Z","shell.execute_reply.started":"2024-11-25T18:10:34.678502Z","shell.execute_reply":"2024-11-25T18:10:48.810382Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pesq in /opt/conda/lib/python3.10/site-packages (0.0.4)\nBuilding codebook...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Codebook built successfully.\nProcessing: /kaggle/input/speechdata/female1.wav\nBitrate: 11000.00 bits/sec\nSignal-to-Noise Ratio (SNR): -16.56 dB\nProcessing: /kaggle/input/speechdata/female2.wav\nBitrate: 10999.83 bits/sec\nSignal-to-Noise Ratio (SNR): -8.98 dB\nProcessing: /kaggle/input/speechdata/male1.wav\nBitrate: 10999.66 bits/sec\nSignal-to-Noise Ratio (SNR): -11.67 dB\nProcessing: /kaggle/input/speechdata/male2.wav\nBitrate: 10999.89 bits/sec\nSignal-to-Noise Ratio (SNR): -10.14 dB\n\nSummary of Results:\nFile: /kaggle/input/speechdata/female1.wav\nRuntime: 0.24 seconds\nSegmental SNR: -14.64 dB\nPESQ Score: 1.2623679637908936\n\nFile: /kaggle/input/speechdata/female2.wav\nRuntime: 0.17 seconds\nSegmental SNR: -6.10 dB\nPESQ Score: 1.3645005226135254\n\nFile: /kaggle/input/speechdata/male1.wav\nRuntime: 0.09 seconds\nSegmental SNR: -9.06 dB\nPESQ Score: 1.3076508045196533\n\nFile: /kaggle/input/speechdata/male2.wav\nRuntime: 0.25 seconds\nSegmental SNR: -7.97 dB\nPESQ Score: 1.2292536497116089\n\n","output_type":"stream"}],"execution_count":13}]}